{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GCD Assignment 5 - MapReduce\n",
    "\n",
    "<b>MapReduce</b> is a processing technique for processing huge amounts of data, in parallel, on large clusters of commodity hardware in a reliable manner.\n",
    "\n",
    "The MapReduce algorithm contains two import tasks, namely <b>Map</b> and <b>Reduce</b>. Map takes a set of data and converts it into another set of data, where individual elements are broken down into tuples(key/value pairs). The Reduce task, takes the output <b>from a map as an input</b> and combines those data tuples into smaller set of tuples. As the sequence of the name MapReduce implies, the reduce task is always performed after the map job.\n",
    "\n",
    "The major advantage of MapReduce is that it is easy to scale data processing over multiple computing nodes. Under the MapReduce model, the data processing primitives are called <b>mappers and reducers</b>.\n",
    "\n",
    "<b>The Algorithm</b>\n",
    "\n",
    "- Generally MapReduce paradigm is based on sending the computer to where the data resides!\n",
    "\n",
    "- MapReduce program executes in three stages, namely map stage, shuffle stage, and reduce stage.\n",
    "\n",
    "- <b>Map stage</b> : The map or mapper’s job is to process the input data. Generally the input data is in the form of file or directory and is stored in the Hadoop file system (HDFS). The input file is passed to the mapper function line by line. The mapper processes the data and creates several small chunks of data.\n",
    "\n",
    "- <b>Reduce stage</b> : This stage is the combination of the Shuffle stage and the Reduce stage. The Reducer’s job is to process the data that comes from the mapper. After processing, it produces a new set of output, which will be stored in the HDFS.\n",
    "\n",
    "- During a MapReduce job, Hadoop sends the Map and Reduce tasks to the appropriate servers in the cluster.\n",
    "\n",
    "- The framework manages all the details of data-passing such as issuing tasks, verifying task completion, and copying data around the cluster between the nodes.\n",
    "\n",
    "- Most of the computing takes place on nodes with data on local disks that reduces the network traffic.\n",
    "\n",
    "- After completion of the given tasks, the cluster collects and reduces the data to form an appropriate result, and sends it back to the Hadoop server.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import mincemeat\n",
    "\n",
    "import glob\n",
    "\n",
    "# allStopWords={'about':1, 'above':1, 'after':1, 'again':1, 'against':1, 'all':1, 'am':1, 'an':1, 'and':1, 'any':1, 'are':1, 'arent':1, 'as':1, 'at':1, 'be':1, 'because':1, 'been':1, 'before':1, 'being':1, 'below':1, 'between':1, 'both':1, 'but':1, 'by':1, 'cant':1, 'cannot':1, 'could':1, 'couldnt':1, 'did':1, 'didnt':1, 'do':1, 'does':1, 'doesnt':1, 'doing':1, 'dont':1, 'down':1, 'during':1, 'each':1, 'few':1, 'for':1, 'from':1, 'further':1, 'had':1, 'hadnt':1, 'has':1, 'hasnt':1, 'have':1, 'havent':1, 'having':1, 'he':1, 'hed':1, 'hell':1, 'hes':1, 'her':1, 'here':1, 'heres':1, 'hers':1, 'herself':1, 'him':1, 'himself':1, 'his':1, 'how':1, 'hows':1, 'i':1, 'id':1, 'ill':1, 'im':1, 'ive':1, 'if':1, 'in':1, 'into':1, 'is':1, 'isnt':1, 'it':1, 'its':1, 'its':1, 'itself':1, 'lets':1, 'me':1, 'more':1, 'most':1, 'mustnt':1, 'my':1, 'myself':1, 'no':1, 'nor':1, 'not':1, 'of':1, 'off':1, 'on':1, 'once':1, 'only':1, 'or':1, 'other':1, 'ought':1, 'our':1, 'ours ':1, 'ourselves':1, 'out':1, 'over':1, 'own':1, 'same':1, 'shant':1, 'she':1, 'shed':1, 'shell':1, 'shes':1, 'should':1, 'shouldnt':1, 'so':1, 'some':1, 'such':1, 'than':1, 'that':1, 'thats':1, 'the':1, 'their':1, 'theirs':1, 'them':1, 'themselves':1, 'then':1, 'there':1, 'theres':1, 'these':1, 'they':1, 'theyd':1, 'theyll':1, 'theyre':1, 'theyve':1, 'this':1, 'those':1, 'through':1, 'to':1, 'too':1, 'under':1, 'until':1, 'up':1, 'very':1, 'was':1, 'wasnt':1, 'we':1, 'wed':1, 'well':1, 'were':1, 'weve':1, 'were':1, 'werent':1, 'what':1, 'whats':1, 'when':1, 'whens':1, 'where':1, 'wheres':1, 'which':1, 'while':1, 'who':1, 'whos':1, 'whom':1, 'why':1, 'whys':1, 'with':1, 'wont':1, 'would':1, 'wouldnt':1, 'you':1, 'youd':1, 'youll':1, 'youre':1, 'youve':1, 'your':1, 'yours':1, 'yourself':1, 'yourselves':1}\n",
    "\n",
    "\n",
    "all_files = glob.glob('Gutenberg Small/*.*')\n",
    "def file_contents(file_name):\n",
    "    f = open(file_name, encoding = 'utf-8', errors = 'ignore')\n",
    "    try:\n",
    "        return f.read()\n",
    "    finally:\n",
    "        f.close()\n",
    "data = dict((file_name, file_contents(file_name)) for file_name in all_files)\n",
    "\n",
    "def mapfn(k, v):\n",
    "    for w in v.split():\n",
    "        yield w, 1\n",
    "\n",
    "def reducefn(k, vs):\n",
    "    result = 0\n",
    "    for v in vs:\n",
    "        result += v\n",
    "    return result\n",
    "\n",
    "s = mincemeat.Server()\n",
    "\n",
    "# The data source can be any dictionary-like object\n",
    "s.datasource = dict(enumerate(data.values()))\n",
    "s.mapfn = mapfn\n",
    "s.reducefn = reducefn\n",
    "\n",
    "results = s.run_server(password=\"changeme\")\n",
    "\n",
    "#sort results from high to low\n",
    "results = sorted(results.items(), key=lambda x: x[1], reverse=True)\n",
    "#print results\n",
    "i = open('outfile','w')\n",
    "i.write(str(results))\n",
    "i.close()\n",
    "\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OUTPUT: [('the', 44296), ('of', 23835), ('and', 20476), ('to', 18147), ('a', 16316), ('I', 12981), ('in', 10014), ('was', 9868), ('that', 8808), ('had', 6240), ('he', 6075), .......]\n",
    "\n",
    "These are the top 10 most used words in the files.\n",
    "\n",
    "<img src=\"output-1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity 2.2\n",
    "\n",
    "In code below we have imported the stopwords python file which contains a dictionary of words such as: <i>'the', 'a', 'about', 'and', 'of' and etc</i>. We execute the import inside the map function since it is explicitly called by the command prompt. Moreover, the core also does not count one-letter words such as 'a', 'I' and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import mincemeat\n",
    "\n",
    "import glob\n",
    "\n",
    "\n",
    "all_files = glob.glob('Gutenberg Small/*.*')\n",
    "def file_contents(file_name):\n",
    "    f = open(file_name, encoding = 'utf-8', errors = 'ignore')\n",
    "    try:\n",
    "        return f.read()\n",
    "    finally:\n",
    "        f.close()\n",
    "data = dict((file_name, file_contents(file_name)) for file_name in all_files)\n",
    "\n",
    "def mapfn(k, v):\n",
    "    import stopwords    # Otherwise throws error, should pass it with the function\n",
    "\n",
    "    for w in v.split():\n",
    "        if(w.lower() not in stopwords.allStopWords and (w.lower().isalnum() and len(w) > 1)):  # checks for one letter words\n",
    "            yield w, 1\n",
    "\n",
    "def reducefn(k, vs):\n",
    "    result = 0\n",
    "    for v in vs:\n",
    "        result += v\n",
    "    return result\n",
    "\n",
    "s = mincemeat.Server()\n",
    "\n",
    "# The data source can be any dictionary-like object\n",
    "s.datasource = dict(enumerate(data.values()))\n",
    "s.mapfn = mapfn\n",
    "s.reducefn = reducefn\n",
    "\n",
    "results = s.run_server(password=\"changeme\")\n",
    "\n",
    "#sort results from high to low\n",
    "results = sorted(results.items(), key=lambda x: x[1], reverse=True)\n",
    "#print results\n",
    "i = open('outfile','w')\n",
    "i.write(str(results))\n",
    "i.close()\n",
    "\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After filtering the words count without one letter words and special characters my output looks like this:\n",
    "\n",
    "<img src=\"output-2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity 2.3\n",
    "\n",
    "After examining the CPU of my computer, I found out that my CPU has 2 physical cores and 4 logical. I would assume that that my PC can handle at maximum 4 clients, but the performance would not be that good as having 2 clients. There can be run be run many more clients as well but the performance will become slower as the clients increase. Below I perform test with 2 and 4 clients to check my assumption.\n",
    "\n",
    "After implementing a code for measuring the time, I tested my application with 2 clients. The output of execution time was 24 sec. Execution time with 4 clients is 140 sec. The time taken has increased almost 6 times. I have tried with 6 clients but it does not execute the last two started. During that I was <b>only using one processor/process.</b>\n",
    "\n",
    "2 clients, one processor:\n",
    "\n",
    "<img src=\"output-3.png\">\n",
    "\n",
    "4 clients, one processor:\n",
    "\n",
    "<img src=\"output-4.png\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
